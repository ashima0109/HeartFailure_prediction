{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss, f1_score\n",
    "\n",
    "class Analysis():\n",
    "\n",
    "    def read_display(self, csvfilepath):\n",
    "        if not os.path.exists(csvfilepath):\n",
    "            logger.error('CSV path not found')\n",
    "        data = pd.read_csv(csvfilepath)\n",
    "        self.data = data\n",
    "        return data.head(10)\n",
    "\n",
    "    def describe(self):\n",
    "        return self.data.describe()\n",
    "\n",
    "\n",
    "    def nullval(self):\n",
    "        return self.data.isnull().sum()\n",
    "\n",
    "\n",
    "    def unique(self, count = True, column = None):\n",
    "        unique, counts = np.unique(self.data[column].values, return_counts=True)\n",
    "\n",
    "        plt.bar(unique,counts)\n",
    "        plt.title('Class Frequency')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "        if count:\n",
    "            return(self.data[column].value_counts())\n",
    "        \n",
    "        \n",
    "    def correlation(self):\n",
    "        corr = self.data.corr()\n",
    "        return corr.style.background_gradient(cmap='coolwarm').set_precision(3)\n",
    "    \n",
    "    \n",
    "    def drop_features(self, features_to_drop):\n",
    "        for f in features_to_drop:\n",
    "            self.data = self.data.drop(f, 1)\n",
    "            \n",
    "        return self.data\n",
    "    \n",
    "    def encode_category(self, features_to_encode):\n",
    "        for f in features_to_encode:\n",
    "            if self.data.dtypes[f] == 'object':\n",
    "                self.data[f] = self.data[f].astype('category')\n",
    "                self.data[f] = self.data[f].cat.codes\n",
    "            else:\n",
    "                logger.debug('{} is not a categorical variable'.format(f))\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    def normalize(self, features_to_normalize):\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        x = self.data[features_to_normalize].values\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "        df_temp = pd.DataFrame(x_scaled, columns=features_to_normalize, index = self.data.index)\n",
    "        self.data[features_to_normalize] = df_temp\n",
    "        \n",
    "        return self.data.head(10)\n",
    "    \n",
    "    def balancesplit(self,Y_column = None,test_ratio = 0.2):\n",
    "        Y = self.data[Y_column]\n",
    "        X = self.data.drop(Y_column, axis = 1)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_ratio, random_state=20) \n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "        \n",
    "    def imbalancesplit(self, Y_column = None, task = None, thresh = 0.6, random = 50, test_ratio = 0.2):\n",
    "        \n",
    "        Y = self.data[Y_column]\n",
    "        X = self.data.drop(Y_column, axis = 1)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_ratio, random_state=20)\n",
    "\n",
    "        train_data = pd.concat([X_train, Y_train], axis=1)\n",
    "\n",
    "        df_1 = train_data[train_data[Y_column]==0]\n",
    "        df_2 = train_data[train_data[Y_column]==1]\n",
    "\n",
    "        if df_1.shape[0] > df_2.shape[0]:\n",
    "            majority,minority = df_1, df_2\n",
    "        else:\n",
    "            majority,minority = df_2, df_1\n",
    "\n",
    "        ratio = minority.shape[0]/majority.shape[0]\n",
    "        if ratio > thresh:\n",
    "            logger.debug('Ratio of minority to majority class is {}'.format(ratio))\n",
    "            return X_train, X_test, Y_train, Y_test\n",
    "            \n",
    "            \n",
    "\n",
    "        if task == 'up':\n",
    "            minority_upsampled = resample(minority,\n",
    "                                         replace = True,\n",
    "                                         n_samples = len(majority),\n",
    "                                         random_state = random)\n",
    "            df = pd.concat([majority, minority_upsampled])\n",
    "        elif task == 'down':\n",
    "            majority_downsampled = resample(majority,\n",
    "                                           replace = False,\n",
    "                                           n_samples = len(minority),\n",
    "                                           random_state = random)\n",
    "            df = pd.concat([majority_downsampled, minority])\n",
    "            \n",
    "        Y_train = df[Y_column]\n",
    "        X_train = df.drop(Y_column, axis = 1)\n",
    "        \n",
    "        print('Train distribution:',Y_train.value_counts())\n",
    "        print('Test distribution:',Y_test.value_counts())\n",
    "\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "    \n",
    "    def logistic(self,x_train, x_test, y_train, y_test):\n",
    "        clf = LogisticRegression(penalty='l2',solver='lbfgs')\n",
    "        return self.plot(clf, x_train, x_test, y_train, y_test)  \n",
    "\n",
    "    def svmclass(self,x_train, x_test, y_train, y_test, kernel = 'linear'):\n",
    "        clf = svm.SVC(kernel=kernel,probability=True) # Linear Kernel\n",
    "        return self.plot(clf, x_train, x_test, y_train, y_test)  \n",
    "    \n",
    "    \n",
    "    def decisiontree(self,x_train, x_test, y_train, y_test, criterion = \"gini\", depth = 8):\n",
    "        clf = DecisionTreeClassifier(criterion=criterion,max_features=\"log2\",max_depth=depth,random_state=6)\n",
    "        return self.plot(clf, x_train, x_test, y_train, y_test)\n",
    "        \n",
    "    def gradientboost(self,x_train, x_test, y_train, y_test, depth = 4):\n",
    "        clf = GradientBoostingClassifier(learning_rate=0.2,max_depth=depth)\n",
    "        return self.plot(clf, x_train, x_test, y_train, y_test)\n",
    "        \n",
    "    \n",
    "    def randomforest(self,x_train, x_test, y_train, y_test, depth = 8):\n",
    "        clf = RandomForestClassifier(max_depth=depth,max_features=\"log2\",random_state=4)\n",
    "        return self.plot(clf, x_train, x_test, y_train, y_test)\n",
    "        \n",
    "        \n",
    "    def XGboost(self, x_train, x_test, y_train, y_test):\n",
    "        clf = XGBClassifier(random_state=6)\n",
    "        return self.plot(clf, x_train, x_test, y_train, y_test)\n",
    "        \n",
    "          \n",
    "    def plot(self,clf, x_train, x_test, y_train, y_test):\n",
    "        # Train Random forest Classifer\n",
    "        clf = clf.fit(x_train,y_train)\n",
    "\n",
    "        #Predict the response for test dataset\n",
    "        y_pred = clf.predict(x_test)\n",
    "\n",
    "        # prediction of the test data\n",
    "        y_pred = clf.predict(x_test)\n",
    "        y_pred_proba = clf.predict_proba(x_test)[:, 1]\n",
    "        [fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "        print('Train/Test split results:')\n",
    "        print(\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "        print(\" precison is %2.3f\" % precision_score(y_test, y_pred))\n",
    "        print(\" recall is %2.3f\" % recall_score(y_test, y_pred))\n",
    "        print(\" auc is %2.3f\" % auc(fpr, tpr))\n",
    "        print(\" f1 score is %2.3f\" % f1_score(y_test, y_pred))\n",
    "#         self.confusion(y_test, y_pred)\n",
    "        return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def confusion(self,y_test, y_pred):\n",
    "        labels = [1,0]\n",
    "\n",
    "        # confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "        ax= plt.subplot()\n",
    "        sns.heatmap(cm, annot=True, fmt =\".0f\",ax = ax); #annot=True to annotate cells\n",
    "\n",
    "        # labels, title and ticks\n",
    "        ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "        ax.set_title('Confusion Matrix'); \n",
    "        ax.xaxis.set_ticklabels([1, 0]); ax.yaxis.set_ticklabels([1, 0])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
